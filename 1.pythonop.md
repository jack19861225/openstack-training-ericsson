#Python 运维相关事宜练习

##Python下自动化管理Mysql数据库

核心知识点在于对mysqldb的使用

- utils.py
```
#!/usr/local/bin/python2.7
#-*-coding:utf-8 -*-
'''
    格式时间的转换，数据库配置文件的单位(*.cnf)和数据库global（mysql>show global variables;）配置的单位不一致，需要转换
'''
 
unit = {'t':2**40,'g':2**30,'m':2**20,'k':2**10,'b':1}
 
def convertUnit(s):
    s = s.lower()
    lastchar = s[-1]
    num = int(s[:-1])
    if lastchar in unit:
        return num*unit[lastchar]
    else:
        return int(s)
 
def scaleUnit(d):
    for k,v in unit.items():
        num = d / v
        if (0 < num < 2**10):
            return num,k
```
- mysql.py

```
#!/usr/local/bin/python2.7
#-*- coding:utf-8 -*-
 
from ConfigParser import ConfigParser
import os
import MySQLdb
 
def getMyVariables(cur):
    '''查询数据库配置信息'''
    cur.execute('show global variables;')
    data = cur.fetchall()
    return dict(data)
 
class MySQLDConfig(ConfigParser):
    '''将所有公用的功能封装成一个class'''
    def __init__(self, config, **kw):
        '''Python版本必须2.7以上，2.6版本没有allow_no_value 属性'''
        ConfigParser.__init__(self, allow_no_value=True)
        self.config = config
        self.mysqld_vars = {}
        if os.path.exists(self.config):
            self.read(self.config)
            self.get_mysqld_vars()
        else:
            self.set_mysqld_defaults_var()
        self.set_mysqld_vars(kw)
 
    def set_mysqld_vars(self, kw):
        '''获取配置文件信息，覆盖默认配置'''
        for k, v in kw.items():
            self.mysqld_vars[k] = v
 
    def get_mysqld_vars(self):
        '''获取现有配置文件信息'''
        options = self.options('mysqld')
        rst = {}
        for o in options:
            rst[o] = self.get('mysqld', o)
        self.set_mysqld_vars(rst)
 
    def set_mysqld_defaults_var(self):
        '''如果配置文件不存在，设置默认配置'''
        defaults = {
            "user":"mysql",
            "pid-file": "/var/run/mysqld/mysqld.pid",
            "socket": "/var/lib/mysql/mysql.sock",
            "port": "3306",
            "basedir": "/usr",
            "datadir": "/tmp/mysql",
            "tmpdir": "/tmp",
            "skip-external-locking": None,
            "bind-address": "127.0.0.1",
            "key_buffer": "16M",
            "max_allowed_packet": "16M",
            "thread_stack": "192K",
            "thread_cache_size": "8",
            "myisam-recover": "BACKUP",
            "query_cache_limit": "1M",
            "query_cache_size": "16M",
            "log_error": "/var/log/mysqld.log",
            "expire_logs_days": "10",
            "max_binlog_size": "100M"
        }
        self.set_mysqld_vars(defaults)
 
    def save(self):
        '''将配置信息保存至配置文件'''
        if not self.has_section('mysqld'):
            self.add_section('mysqld')
        for k, v in self.mysqld_vars.items():
            self.set('mysqld', k, v)
        with open(self.config, 'w') as fd:
            self.write(fd)
 
if __name__  == "__main__":
    mc = MySQLDConfig('/root/david/mysqlmanager/cnfs/my.cnf', max_connection=200, user='mysql')
    mc.set_var('skip-slave-start', None)
    mc.save()
```

- manage.py

```

#!/usr/local/bin/python2.7
#-*- coding:utf-8 -*-
 
from os import path
from optparse import OptionParser
from subprocess import PIPE, Popen
import MySQLdb
import glob
import os
import sys
import time
import datetime
import re
 
 
DIRNAME = path.dirname(__file__)
OPSTOOLS_DIR = path.abspath(path.join(DIRNAME, '..'))
sys.path.append(OPSTOOLS_DIR)
from library.mysql import MySQLDConfig, getMyVariables
 
REPLICATION_USER = 'repl'
REPLICATION_PASS = '123qwe'
MYSQL_DATA_DIR = '/home/david/data'
MYSQL_CONF_DIR = '/home/david/cnfs'
MYSQL_BACK_DIR = '/home/david/backup'
 
def opts():
    parser = OptionParser(usage="usage: %prog [options] arg1 arg2")
    parser.add_option("-c","--cmd",
        dest="cmd",
        action="store",
        default="check",
        help="Check the configuration file and database configuration parameters are different.[%options]"
    )
    parser.add_option("-n","--name",
        dest="name",
        action="store",
        default="mysqlinstance",
        help="Create Examples."
    )
    parser.add_option("-p","--port",
        dest="port",
        action="store",
        default="3306",
        help="Examples of port."
    )
    return parser.parse_args()
 
def checkPort(d, p):
    '''实例端口检测'''
    for m in d:
        if p == m.mysqld_vars['port']:
            return True
    return False
 
def setReplMaster(cur):
    '''设置slave数据库同步用户的授权'''
    sql = "GRANT REPLICATION SLAVE ON *.* TO %s@'localhost' IDENTIFIED BY '%s'" % (REPLICATION_USER, REPLICATION_PASS)
    cur.execute(sql)
 
def connMySQLd(mc):
    '''连接数据库'''
    host = '127.0.0.1'
    user = 'root'
    port = int(mc.mysqld_vars['port'])
    conn = MySQLdb.connect(host, port=port, user=user)
    cur = conn.cursor()
    return cur
 
def run_mysql(cnf):
    '''运行数据库'''
    cmd = "mysqld_safe --defaults-file=%s &" % cnf
    p = Popen(cmd, stdout=PIPE, shell=True)
    time.sleep(5)
    return p.returncode
 
def setOwner(p, user):
    '''设置目录权限'''
    os.system("chown -R %s:%s %s" % (user, user, p))
 
def mysql_install_db(cnf):
    '''数据库初始化'''
    p = Popen("mysql_install_db --defaults-file=%s" % cnf, stdout=PIPE, shell=True)
    #p = Popen("mysql_install_db --user=mysql --datadir=%s " % MYSQL_DATA_DIR, stdout=PIPE, shell=True)
    stdout, stderr = p.communicate()
    return p.returncode
 
def _genDict(name, port):
    '''设置文件存储目录及监听端口'''
    return {
        'pid-file': path.join(MYSQL_DATA_DIR, name, "%s.pid" % name),
        'socket': '/tmp/%s.sock' % name,
        'port': port,
        'datadir': path.join(MYSQL_DATA_DIR, name)+'/',
        'log_error': path.join(MYSQL_DATA_DIR, name)
    }
 
def readConfs():
    '''读取配置文件，如果配置文件不存在，使用默认配置生成配置文件'''
    confs = glob.glob(path.join(MYSQL_CONF_DIR, '*.cnf'))
    return [MySQLDConfig(c) for c in confs]
 
def getCNF(name):
    '''获取配置文件完整路径'''
    return path.join(MYSQL_CONF_DIR, "%s.cnf" % name)
 
def runMySQLdump(cmd):
    '''启动Mysql命令'''
    p = Popen(cmd, stdout=PIPE, shell=True)
    stdout, stderr = p.communicate()
    return p.returncode
 
def getBinlogPOS(f):
    '''获取binlog'''
    with open(f) as fd:
        f, p = findLogPos(l)
        if f and p:
            return f,p
 
def findLogPos(s):
    rlog = re.compile(r"MASTER_LOG_FILE='(\S+)',", re.IGNORECASE)
    rpos = re.compile(r"MASTER_LOG_POS=(\d+),?", re.IGNORECASE)
    log = rlog.search(s)
    pos = rpos.search(s)
    if log and pos:
        return log.group(1), int(pos.group(1))
    else:
        return (None, None)
 
def changeMaster(cur, host, port, user, mpass, mf, p):
    sql = '''CHANGE MASTER TO
        MASTER_HOST='%s',
        MASTER_PORT='%s',
        MASTER_USER='%s',
        MASTER_PASSWORD='%s',
        MASTER_LOG_FILE='%s',
        MASTER_LOG_POS=%s;''' % (host, port, user, mpass, mf, p)
        cur.execute(sql)
 
def createInstance(name, port, dbtype="master", **kw):
    '''创建数据库实例'''
    cnf = path.join(MYSQL_CONF_DIR, "%s.cnf" % name)
    datadir = path.join(MYSQL_DATA_DIR, name)
    exists_cnfs = readConfs()
 
    if checkPort(exists_cnfs, port):
        print >> sys.stderr, "port exist."
        sys.exit(-1)
    if not path.exists(cnf):
        c = _genDict(name, port)
        c.update(kw)
        mc = MySQLDConfig(cnf, **c)
        mc.save()
    else:
        mc = MySQLDConfig(cnf, **kw)
 
    if not path.exists(datadir):
        mysql_install_db(cnf)
        setOwner(datadir, mc.mysqld_vars['user'])
        run_mysql(cnf)
        time.sleep(3)
        cur = connMySQLd(mc)
        setReplMaster(cur)
 
def diffVariables(instance_name):
    '''查询数据库配置文件和数据库配置的差异'''
    cnf = getCNF(instance_name)
    if path.exists(cnf):
        mc = MySQLDConfig(cnf)
        print mc
        cur = connMySQLd(mc)
        vars = getMyVariables(cur)
        for k, v in mc.mysqld_vars.items():
            k = k.replace('-', '_')
            if k in vars and vars[k] != v:
                print k, v, vars[k]
 
def setVariable(instance_name, variable, value):
    '''重新加载配置'''
    cnf = getCNF(instance_name)
    if path.exists(cnf):
        mc = MySQLDConfig(cnf)
        cur = connMySQLd(mc)
        cur.execute('set global %s = %s' % (variable, value))
        mc.set_var(variable, value)
        mc.save()
 
def backupMySQL(instance_name):
    '''备份数据库'''
    cnf = getCNF(instance_name)
    if path.exists(cnf):
        mc = MySQLDConfig(cnf)
    now = datetime.datetime.now()
    timestamp = now.strftime('%Y-%m-%d-%H%M%S')
    backup_file = path.join(MYSQL_BACK_DIR, instance_name, timestamp+'.sql')
    _dir = path.dirname(backup_file)
    if not path.exists(_dir):
        os.makedirs(_dir)
    cmd = 'mysqldump -A -x -F --master-data=1 --host=127.0.0.1 --user=root --port=%s > %s' % (mc.mysqld_vars['port'], backup_file)
    runMySQLdump(cmd)
 
def restoreMySQL(instance_name, instance_port, sqlfile, **kw):
    createInstance(instance_name, instance_port, **kw)
    cnf = getCNF(instance_name)
    if path.exists(cnf):
        mc = MySQLDConfig(cnf)
        cur = connMySQLd(mc)
        cmd = "mysql -h 127.0.0.1 -P %s -u root < %s" % (mc.mysqld_vars['port'], sqlfile)
        f, p = getBinlogPOS(sqlfile)
        runMySQLdump(cmd)
        changeMaster(cur, 
                     host=kw['master-host'],
                     port=kw['master-port'],
                     user=REPLICATION_USER,
                     mpass=REPLICATION_PASS,
                     mf=f, 
                     p=p)
 
def _init():
    '''查询mysql几个目录是否存在，如果不存在，自动创建'''
    if not path.exists(MYSQL_DATA_DIR):
        os.makedirs(MYSQL_DATA_DIR)
    if not path.exists(MYSQL_CONF_DIR):
        os.makedirs(MYSQL_CONF_DIR)
    if not path.exists(MYSQL_BACK_DIR):
        os.makedirs(MYSQL_BACK_DIR)
 
def main():
    opt, args = opts()
    instance_name = opt.name
    instance_port = opt.port
    command = opt.cmd
    if command == "create":
        if not args:
            createInstance(instance_name, instance_port)
        else:
            dbtype = args[0]
            serverid = args[1]
            mysqld_options = {'server-id':serverid}
            if dbtype == 'master':
                mysqld_options['log-bin'] = 'mysql-bin'
            elif dbtype == 'slave':
                master_host = args[2]
                master_port = args[3]
                mysqld_options['master-host'] = master_host
                mysqld_options['master-port'] = master_port
                mysqld_options['master-user'] = REPLICATION_USER
                mysqld_options['master-password'] = REPLICATION_PASS
                mysqld_options['skip-slave-start'] = None
                mysqld_options['replicate-ignore-db'] = 'mysql'
                mysqld_options['read-only'] = None
            createInstance(instance_name, instance_port, dbtype=dbtype, **mysqld_options)
    elif command == 'check':
        diffVariables(instance_name)
    elif command == 'adjust':
        variable = args[0]
        value = args[1]
        setVariable(instance_name, variable, value)
    elif command == 'backup':
        backupMySQL(instance_name)
    elif command == 'restore':
        serverid == args[0]
        mhost = args[1]
        mport = args[2]
        sqlfile = args[3]
        mysqld_options = {
            "master-host":mhost,
            "master-port":mport,
            "server-id":serverid,
            "skip-slave-start":None,
        }
        restoreMySQL(instance_name, instance_port, sqlfile, **mysqld_options)
 
if __name__ == "__main__":
    print main()
```    



## web服务器巡检

-- 一个小脚本
```
#!/usr/bin/env python
#coding=utf8
  
"""
使用socket方式来检查服务器的监控状况
"""
  
from optparse import OptionParser
  
import socket
  
import sys
  
import re
  
from StringIO import StringIO
  
class check_server:
  """
  该类主要是利用socket建立一个连接以后，发送一个http请求，然后根据返回的状态码，判断主机的健康状况
  """
  def __init__(self,address,port,resource):
    self.address = address
    self.port = port
    self.resource = resource
  
  
  def check(self):
    """
    该方法也是该类的主要方法，包括构建请求资源，解析返回结果等
    """
    if not self.resource.startswith('/'):
      self.resource = '/' + self.resource
  
    request = "GET %s HTTP/1.1\r\nHost:%s\r\n\r\n" %(self.resource,self.address)
  
    #建立一个socket连接
  
    s = socket.socket()
    #设置连接超时时间
    s.settimeout(10)
  
    print "现在开始对 %s 上的 %s 端口连接......" %(self.address,self.port)
  
    try:
      s.connect((self.address,self.port))
      print "连接 %s 上端口 %s 成功" %(self.address,self.port)
      s.send(request)
      response = s.recv(100)
  
    except socket.error,e:
      print "连接%s 上端口 %s 失败 ,原因为:%s" %(self.address,self.port,e)
      return False
    finally:
      print "关闭连接"
      s.close()
  
  
    line = StringIO(response).readline()
  
    try:
      (http_version,status,messages) = re.split(r'\s+',line,2)
    except ValueError:
      print "分割响应码失败"
      return False
    print "返回的状态码是%s" %(status)
  
    if status in ['200','301','302']:
      print "服务器的监控状况良好"
    else:
      print "出现问题"
  
  
if __name__ == '__main__':
  """
  处理参数
  """
  parser =OptionParser()
  parser.add_option("-a","--address",dest="address" ,default='localhost',help="要检查主机的地址或者主机名")
  parser.add_option('-p','--port',dest="port",type=int,default=80,help="要检查主机的端口")
  parser.add_option('-r','--resource',dest="resource",default="/",help="要检查的资源，比如")
  (options,args) = parser.parse_args()
  
#开始检测
checks = check_server(options.address,options.port,options.resource)
  
checks.check()

```

-- 复杂脚本
```
#!/usr/bin/env python
# coding=utf-8
#----------------------------------------------------------
# Name:         WEB服务器巡检脚本
# Purpose:      监控多台Web服务器状态，一旦出现问题就发送邮件
# Version:      1.0
#----------------------------------------------------------
from smtplib import SMTP
from email import MIMEText
from email import Header
from datetime import datetime
import httplib
#定义要检测的服务器，URL 端口号 资源名称
web_servers = [('192.168.1.254', 80, 'index.html'),
               ('www.xxx.com', 80, 'index.html'),
               ('114.114.114.114', 9000, '/main/login.html'),
              ]
#定义主机 帐号 密码 收件人 邮件主题
smtpserver = 'smtp.163.com'
sender = 'xxxx@xxx.com'
password = 'password'
receiver = ('收件人1','收件人2')
subject = u'WEB服务器告警邮件'
From = u'Web服务器'
To = u'服务器管理员'
#定义日志文件位置
error_log = '/tmp/web_server_status.txt'
def send_mail(context):
    '''发送邮件'''
                                
    #定义邮件的头部信息
    header = Header.Header
    msg = MIMEText.MIMEText(context,'plain','utf-8')
    msg['From'] = header(From)
    msg['To'] = header(To)
    msg['Subject'] = header(subject + '\n')
    #连接SMTP服务器，然后发送信息
    smtp = SMTP(smtpserver)
    smtp.login(sender, password)
    smtp.sendmail(sender, receiver, msg.as_string())
    smtp.close()
def get_now_date_time():
    '''获取当前的日期'''
    now = datetime.now()
    return str(now.year) + "-" + str(now.month) + "-" \
           + str(now.day) + " " + str(now.hour) + ":" \
           + str(now.minute) + ":" + str(now.second)
def check_webserver(host, port, resource):
    '''检测WEB服务器状态'''
    if not resource.startswith('/'):
        resource = '/' + resource
    try:
        try :
            connection = httplib.HTTPConnection(host, port)
            connection.request('GET', resource)
            response = connection.getresponse()
            status = response.status
            content_length = response.length
        except :
            return  False
    finally :
        connection.close()
    if status in [200,301] and content_length != 0:
        return True
    else:
        return False
if __name__ == '__main__':
    logfile = open(error_log,'a')
    problem_server_list = []
    for host in web_servers:
        host_url = host[0]
        check = check_webserver(host_url, host[1], host[2])
        if not check:
            temp_string = 'The Server [%s] may appear problem at %s\n' % (host_url,get_now_date_time())
            print >> logfile, temp_string
            problem_server_list.append(temp_string)
    logfile.close()
    #如果problem_server_list不为空，就说明服务器有问题，那就发送邮件
    if problem_server_list:
        send_mail(''.join(problem_server_list))

```

-- 监控cpu
```
#!/usr/bin/env Python
from __future__ import print_function
from collections import OrderedDict
import pprint

def CPUinfo():
    ''' Return the information in /proc/CPUinfo
    as a dictionary in the following format:
    CPU_info['proc0']={...}
    CPU_info['proc1']={...}
    '''
    CPUinfo=OrderedDict()
    procinfo=OrderedDict()

    nprocs = 0
    with open('/proc/CPUinfo') as f:
        for line in f:
            if not line.strip():
                # end of one processor
                CPUinfo['proc%s' % nprocs] = procinfo
                nprocs=nprocs+1
                # Reset
                procinfo=OrderedDict()
            else:
                if len(line.split(':')) == 2:
                    procinfo[line.split(':')[0].strip()] = line.split(':')[1].strip()
                else:
                    procinfo[line.split(':')[0].strip()] = ''
            
    return CPUinfo

if __name__=='__main__':
    CPUinfo = CPUinfo()
    for processor in CPUinfo.keys():
        print(CPUinfo[processor]['model name'])
```

##python分析nginx大日志文件


```
#log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
'$status $body_bytes_sent "$http_referer" '
'"$http_user_agent" "$http_x_forwarded_for"';

$remote_addr客户端的ip地址（如果中间有代理服务器那么这里显示的ip就为代理服务器的ip地址）
$remote_user用于记录远程客户端的用户名称（一般为“-”）
$time_local用于记录访问时间和时区
$request用于记录请求的url以及请求方法$status响应状态码
$body_bytes_sent给客户端发送的文件主体内容大小
$http_user_agent用户所使用的代理（一般为浏览器）
$http_x_forwarded_for可以记录客户端IP，通过代理服务器来记录客户端的ip地址
$http_referer可以记录用户是从哪个链接访问过来的

demo data

182.19.31.129 - - [2013-08-13T00:00:01-07:00] "GET /css/anniversary.css HTTP/1.1" 304 0 "http://www.chlinux.net/" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.95 Safari/537.36" "-"

```

- 进行一些基本的统计
```
  
import os  
import fileinput  
import re  
  
#日志的位置  
dir_log  = r"D:\python cmd\nginxlog"  
  
#使用的nginx默认日志格式$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" "$http_x_forwarded_for"'  
#日志分析正则表达式  
  
#203.208.60.230   
ipP = r"?P<ip>[\d.]*";  
  
#[21/Jan/2011:15:04:41 +0800]  
timeP = r"""?P<time>\[           #以[开始 
            [^\[\]]* #除[]以外的任意字符  防止匹配上下个[]项目(也可以使用非贪婪匹配*?)  不在中括号里的.可以匹配换行外的任意字符  *这样地重复是"贪婪的“ 表达式引擎会试着重复尽可能多的次数。 
            \]           #以]结束 
        """  
  
#"GET /EntpShop.do?method=view&shop_id=391796 HTTP/1.1"  
requestP = r"""?P<request>\"          #以"开始 
            [^\"]* #除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?) 
            \"          #以"结束 
            """  
  
statusP = r"?P<status>\d+"  
  
bodyBytesSentP = r"?P<bodyByteSent>\d+"  
  
#"http://test.myweb.com/myAction.do?method=view&mod_id=&id=1346"  
referP = r"""?P<refer>\"          #以"开始 
            [^\"]* #除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?) 
            \"          #以"结束 
        """  
  
#"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"'  
userAgentP = r"""?P<userAgent>\"              #以"开始 
        [^\"]* #除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?) 
        \"              #以"结束 
            """  
  
#原理：主要通过空格和-来区分各不同项目，各项目内部写各自的匹配表达式  
nginxLogPattern = re.compile(r"(%s)\ -\ -\ (%s)\ (%s)\ (%s)\ (%s)\ (%s)\ (%s)" %(ipP, timeP, requestP, statusP, bodyBytesSentP, referP, userAgentP), re.VERBOSE)  
  
def processDir(dir_proc):  
    for file in os.listdir(dir_proc):  
        if os.path.isdir(os.path.join(dir_proc, file)):  
            print "WARN:%s is a directory" %(file)  
            processDir(os.path.join(dir_proc, file))  
            continue  
  
        if not file.endswith(".log"):  
            print "WARN:%s is not a log file" %(file)  
            continue  
  
        print "INFO:process file %s" %(file)  
        for line in fileinput.input(os.path.join(dir_proc, file)):  
            matchs = nginxLogPattern.match(line)  
            if matchs!=None:  
                allGroups = matchs.groups()  
                ip = allGroups[0]  
                time = allGroups[1]  
                request = allGroups[2]  
                status =  allGroups[3]  
                bodyBytesSent = allGroups[4]  
                refer = allGroups[5]  
#                userAgent = allGroups[6]  
                userAgent = matchs.group("userAgent")  
                print userAgent  
                  
                #统计HTTP状态码的数量  
                GetResponseStatusCount(userAgent)  
                #在这里补充其他任何需要的分析代码  
            else:  
                raise Exception  
                  
        fileinput.close()  
  
allStatusDict = {}  
#统计HTTP状态码的数量  
def GetResponseStatusCount(status):  
    if allStatusDict.has_key(status):  
        allStatusDict[status] += 1;  
    else:  
        allStatusDict[status] = 1;  
      
          
if __name__ == "__main__":  
    processDir(dir_log)  
    print allStatusDict  
    #根据值进行排序（倒序）  
    print sorted(allStatusDict.items(), key=lambda d:d[1], reverse=True)  
    print "done, python is great!"  
```

- 塞入mysql

``` 
#!/usr/bin/env python
#coding:utf8
import os
import fileinput
import re
import sys
import MySQLdb
#日志的位置
logfile=open("access_20130812.log")
#使用的nginx默认日志格式$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" "$http_x_forwarded_for"'
#日志分析正则表达式
#203.208.60.230
ipP = r"?P<ip>[\d.]*"
#以[开始,除[]以外的任意字符 防止匹配上下个[]项目(也可以使用非贪婪匹配*?) 不在中括号里的.可以匹配换行外的任意字符 *这样地重复是"贪婪的“ 表达式引擎会试着重复尽可能多的次数。#以]结束
#[21/Jan/2011:15:04:41 +0800]
timeP = r"""?P<time>\[[^\[\]]*\]"""
#以"开始, #除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?),#以"结束
#"GET /EntpShop.do?method=view&shop_id=391796 HTTP/1.1"
#"GET /EntpShop.do?method=view&shop_id=391796 HTTP/1.1"
requestP = r"""?P<request>\"[^\"]*\""""
statusP = r"?P<status>\d+"
bodyBytesSentP = r"?P<bodyByteSent>\d+"
#以"开始, 除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?),#以"结束
#"http://test.myweb.com/myAction.do?method=view&mod_id=&id=1346"
referP = r"""?P<refer>\"[^\"]*\""""
#以"开始, 除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?),以"结束
#"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"'
userAgentP = r"""?P<userAgent>\"[^\"]*\""""
#以(开始, 除双引号以外的任意字符 防止匹配上下个()项目(也可以使用非贪婪匹配*?),以"结束
#(compatible; Googlebot/2.1; +http://www.google.com/bot.html)"'
userSystems = re.compile(r'\([^\(\)]*\)')
#以"开始，除双引号以外的任意字符防止匹配上下个""项目(也可以使用非贪婪匹配*?),以"结束
userlius = re.compile(r'[^\)]*\"')
#原理：主要通过空格和-来区分各不同项目，各项目内部写各自的匹配表达式
nginxLogPattern = re.compile(r"(%s)\ -\ -\ (%s)\ (%s)\ (%s)\ (%s)\ (%s)\ (%s)" %(ipP, timeP, requestP, statusP, bodyBytesSentP, referP, userAgentP), re.VERBOSE)
#数据库连接信息
conn=MySQLdb.connect(host='192.168.1.22',user='test',passwd='pass',port=3306,db='python')
cur=conn.cursor()
sql = "INSERT INTO python.test VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s)"
while True:
    line = logfile.readline()
    if not line:break
    matchs = nginxLogPattern.match(line)
    if matchs != None:
        allGroup = matchs.groups()
        ip = allGroup[0]
        time = allGroup[1]
        request = allGroup[2]
        status = allGroup[3]
        bodyBytesSent = allGroup[4]
        refer = allGroup[5]
        userAgent = allGroup[6]
        Time = time.replace('T',' ')[1:-7]
        if len(userAgent) > 20:
            userinfo = userAgent.split(' ')
            userkel =  userinfo[0]
            try:
                usersystem = userSystems.findall(userAgent)
                usersystem = usersystem[0]
                print usersystem
                userliu = userlius.findall(userAgent)
                value = [ip,Time,request,status,bodyBytesSent,refer,userkel,usersystem,userliu[1]]
                conn.commit()
                print value
            except IndexError:
                userinfo = userAgent
                value = [ip,Time,request,status,bodyBytesSent,refer,userinfo,"",""]
        else:
            useraa = userAgent
            value = [ip,Time,request,status,bodyBytesSent,refer,useraa,"",""]
    try:
        result = cur.execute(sql,value)
        #conn.commit()
        print result
    except MySQLdb.Error,e:
        print "Mysql Error %d: %s" % (e.args[0], e.args[1])
conn.commit()
conn.close()
```

- 复杂实例

```
#!/usr/bin/python
#coding:utf8
import threading     #载入多线程模块
import time          #载入时间模块
import os            #载入os模块
import shutil        #载入shutil模块
import re            #载入re正则模块
fuhao=os.linesep     #换行符
start_time=int(time.strftime('%H%M%S'))  #执行程序开始时间
print start_time
def count_cpu_heshu():                      #统计cpu核数函数
    file=open('/proc/cpuinfo')
    cpu_sum=[]
    for line in file.readlines():
        cpu_he=re.findall('^processor',line)
        if len(cpu_he)==1:
           cpu_sum.append(cpu_he)
        else:
           continue
    file.close()
    return len(cpu_sum)                    #返回cpu函数
def count_memory_size():                   #统计系统内存大小函数
    mem_sum=int(os.popen("free -m|awk  '{print $2}'|sed -n '2p'").readline().strip())  #统计内存的shell
    return mem_sum                                                                     #返回内存大小
def nginx_log_fenge():                    #因nginx日志太大，需要按500M分割，建立此函数进行分割日志
    if os.path.exists('/data/logs/nginx_tmp/')!=True:       #分割日志的临时目录
       os.makedirs('/data/logs/nginx_tmp/')
    if os.path.exists('/data/logs/nginx_tmp_binfa/')!=True: #并发目录
       os.makedirs('/data/logs/nginx_tmp_binfa/')
    if os.path.exists('/data/logs/nginx_tmp_txt01/')!=True:  #time记录txt目录 
       os.makedirs('/data/logs/nginx_tmp_txt01/')
    if os.path.exists('/data/logs/nginx_tmp_txt02/')!=True:  #url记录txt目录 
       os.makedirs('/data/logs/nginx_tmp_txt02/')
    if os.path.exists('/data/logs/nginx_tmp_chuli/')!=True:  #处理所有txt目录
       os.makedirs('/data/logs/nginx_tmp_chuli/')
    nginx_log_name=os.listdir('/data/logs/nginx_log')[0].strip()  #切割日志名
    nginx_file='/data/logs/nginx_log/%s' %nginx_log_name           #切割日志名及路径
    file=open(nginx_file)                   #nginx日志文件路径
    sizehint=int(count_memory_size() / count_cpu_heshu() * 0.5 * 1024 * 1024)              #此数字是按字节进行计算，这里大小为内存除以cpu核数剩以0.5得到的结果为500M
    position=0                               #当前位置为0
    file_num=1                               #分割文件名默认加1
    while True:                             #当条件为真执行，为假退出。
          lines = file.readlines(sizehint)   #读文件
          file_name='/data/logs/nginx_tmp/dd_access%d' %file_num  #分割成功的文件名
          file01=open(file_name,'w')       #写文件
          file01.writelines(lines)         #把读取的单个1G文件写到文件名中
          if  file.tell() - position > 0: #如果分割的位置大于默认位置就继续执行，否则就退出。
              position = file.tell()      #替换位置
              file_num=file_num+1
              continue
          else:
              break
    file.close()
    file01.close()
    os.remove(file_name)
    time.sleep(300)
def nginx_log_time_count(file_name01):         #nginx分析日志函数
    file_name='/data/logs/nginx_tmp_binfa/%s' %file_name01    #并发日志名
    file_txt='/data/logs/nginx_tmp_txt01/%s.txt' %file_name01        #执行shell结果保存目录
    cmd="awk '{print $4}' %s|awk -F / '{print $NF}'|awk -F : '{print $2$3$4$5}'  2>/dev/null|sort 2>/dev/null|uniq -c 2>/dev/null|sort -nr 2>/dev/null|head -n 1 > %s" %(file_name,file_txt)  #分析脚本
    os.system(cmd)  #执行shell命令
    fuhao_cmd='%s' %fuhao
    f=open(file_txt)
    f1=open('/data/logs/nginx_tmp_chuli/time_sum.txt','a')
    for line in f.readlines():
         time_single_max= line.split()[0]   #单个文件连接数
         f1.writelines(time_single_max)
         f1.write(fuhao_cmd)
    f.close()
    f1.close()
def nginx_log_url_count(file_name01):         #nginx分析日志函数
    file_name='/data/logs/nginx_tmp_binfa/%s' %file_name01  #并发日志名
    file_txt='/data/logs/nginx_tmp_txt02/%s.txt' %file_name01   #执行shell结果保存目录
    cmd="awk '{print $7}' %s  2>/dev/null|sort 2>/dev/null|uniq -c 2>/dev/null|sort -rn 2>/dev/null|head -n 200 > %s " %(file_name,file_txt)  #分析脚本
    os.system(cmd)  #执行shell命令
    fuhao_cmd='%s' %fuhao
    f=open(file_txt)
    f1=open('/data/logs/nginx_tmp_chuli/url_sum.txt','a')
    for line in f.readlines():          #把url_status里面每一行值以列表方法增加到url_count列表里面
        f1.writelines(line.strip())
        f1.write(fuhao_cmd)
    f.close()
    f1.close()
def dxc_call_time_count():                       #多线程调用分析日志函数
    file_name_read=[]                        #文件名读取列表
    f=os.listdir('/data/logs/nginx_tmp_binfa/')   #显示data/logs/nginx_tmp/目录下所有文件
    for read_filename in f:
        filename_chuancan=read_filename.strip()  #单个文件名
        filename=threading.Thread(target=nginx_log_time_count,args=(filename_chuancan,))  #建立多线程
        file_name_read.append(filename)         #添加线程到file_name_read列表
    filename_sum=range(len(file_name_read))     #统计文件名数量
    for line in filename_sum:
        file_name_read[line].start()            #启动线程
    for line in filename_sum:
        file_name_read[line].join()             #等待多线程结束后，就结束进程。
def dxc_call_url_count():                       #多线程调用分析日志函数
    file_name_read=[]                        #文件名读取列表
    f=os.listdir('/data/logs/nginx_tmp_binfa/')   #显示data/logs/nginx_tmp/目录下所有文件
    for read_filename in f:
        filename_chuancan=read_filename.strip()  #单个文件名
        filename=threading.Thread(target=nginx_log_url_count,args=(filename_chuancan,))  #建立多线程
        file_name_read.append(filename)         #添加线程到file_name_read列表
    filename_sum=range(len(file_name_read))     #统计文件名数量
    for line in filename_sum:
        file_name_read[line].start()            #启动线程
    for line in filename_sum:
        file_name_read[line].join()             #等待多线程结束后，就结束进程。
def time_count_chuli():                         #time处理函数
    f=open('/data/logs/nginx_tmp_chuli/time_sum.txt')
    time_max=[]
    for count in f:
       time_max.append(int(count.strip()))
    f.close()
    return max(time_max)
def url_count_chuli():                          #url处理函数
    f=open('/data/logs/nginx_tmp_chuli/url_sum.txt')
    url_max=[]
    for count in f:
        url_max.append(count.split())
    f.close()
    return url_max
def write_report_email():                       #写文件用来发email
    fuhao_cmd='%s' %fuhao
    time_max=time_count_chuli()                 #接受time处理函数返回的结果
    url_max=url_count_chuli()                   #接受url处理函数返回的结果
    file=open('/data/logs/nginx_log_email_tmp.txt','w')
    file.write("nginx单秒的最大请求数为:%d" %time_max)
    file.write(fuhao_cmd)
    file.write('nginx连接数TOP100排序')
    file.write(fuhao_cmd)
    new_dict={}                       #定义一字典用来统计连接重复数,得到字典结果为连接地址:连接重复数
    for i in url_max:                 #遍历url_max列表
         new_dict[i[1]] = new_dict.get(i[1],0) + int(i[0])    #i[1]表示连接地址,i[0]表示连接重复数,new_dict[i[1]]表示把列表中的地址重复数与地址连接交换,如果连接相同，就累加连接重复数.
    n_dict = {}                       #定义一字典用来恢复原来的连接重复数:连接地址
    for k in new_dict:                #遍历new_dict字典
         n_dict[new_dict[k]] = k      #k表示连接地址,new_dict[k]表示连接重复数,最后n_dict结果为连接重复数:连接地址
    url_count=sorted(n_dict.iteritems(),key=lambda dict1:dict1[0],reverse=True) #对字典进行排序
    for line in url_count:
        file.write('连接重复数:')
        file.write(str(line[0]))              #把连接重复数写到日志临时文件
        file.write('   ')
        file.write('http://d.m1905.com')      #写连接头文件
        file.write(str(line[1]))              #把连接地址写到日志临时文件
        file.write(fuhao_cmd)
    file.close()
    file=open('/data/logs/nginx_log_email_tmp.txt','r')   #读取日志临时文件
    row=0
    file01=open('/data/logs/nginx_log_email.txt','w')     #写文件
    for line in file.readlines():
        row=row+1                                         #row表示行数
        if row <= 102:                                    #读取文件到102行，大于102行就退出
           file01.write(line)
        else:
           break
    file.close()
    file01.close()
    os.remove('/data/logs/nginx_log_email_tmp.txt')       #删除日志临时文件  
    os.remove('/data/logs/nginx_tmp_chuli/time_sum.txt')       #删除time_sum文件 
    os.remove('/data/logs/nginx_tmp_chuli/url_sum.txt')       #删url_sum文件  
def rmdir_nginx_log_mulu():       #清空日志目录函数
    shutil.rmtree('/data/logs/nginx_tmp/')  #清空日志临时目录，供新日志存放
    os.mkdir('/data/logs/nginx_tmp/')
    shutil.rmtree('/data/logs/nginx_log/')  #清空日志目录，供新日志存放
    os.mkdir('/data/logs/nginx_log')
def main():
   shutil.rmtree('/data/logs/nginx_tmp_chuli')  #清空日志临时目录，供新日志存放
   os.mkdir('/data/logs/nginx_tmp_chuli')
   cpu_he=count_cpu_heshu()                                       #cpu核数
   while len(os.listdir('/data/logs/nginx_tmp/'))>0:   #动态统计分割日志文件个数
 f=os.listdir('/data/logs/nginx_tmp/')          #动态统计分割日志文件
 key=0                                                       #默认key为0
 while (key<=cpu_he-1 and key<len(f)):      #对cpu核数进行对比
         name = '/data/logs/nginx_tmp/%s' %f[key]    #日志文件名
         shutil.move(name,'/data/logs/nginx_tmp_binfa/')  #移动日志文件，为了减少负载太高
         key=key+1
 dxc_call_time_count()
        dxc_call_url_count() 
 shutil.rmtree('/data/logs/nginx_tmp_binfa/')
 os.mkdir('/data/logs/nginx_tmp_binfa/')
        shutil.rmtree('/data/logs/nginx_tmp_txt01/')
        os.mkdir('/data/logs/nginx_tmp_txt01/')
        shutil.rmtree('/data/logs/nginx_tmp_txt02/')
        os.mkdir('/data/logs/nginx_tmp_txt02/')
   write_report_email()
   rmdir_nginx_log_mulu() 
nginx_log_fenge()
main()
stop_time=int(time.strftime('%H%M%S'))
print stop_time
```

## 多线程

- 多线程socket server

```
#coding:utf-8
import socket
import sys
import time
import Queue
import threading

host = 'localhost'
port = 8000

#创建socket对象
s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)

#绑定一个特定地址，端口
try:
    s.bind((host,port))
except  Exception as e :
    print 'Bind Failed:%s'%(str(e))
    sys.exit()
print 'Socket bind complete!!'

#监听连接
s.listen(10) #最大连接数10

#创建连接队列
queue = Queue.Queue()

#创建线程
class TaskThread(threading.Thread):
    def __init__(self):
        threading.Thread.__init__(self)
        
    def run(self):
        while 1:
            t = queue.get()
            t.send('welecome.....')
            #接收数据
            client_data = t.recv(1024)
            t.sendall(client_data)
            #释放资源
            #t.close()            
        

#接受连接
while 1:

    #将连接放入队列
    conn,addr = s.accept()
    print 'Connected from %s:%s'%(addr[0],str(addr[1]))
    queue.put(conn)

    #生成线程池
    th = TaskThread()
    th.setDaemon(True)
    th.start()

    queue.join()
s.close()

```

- 多线程下载图片
```
#coding:utf-8

'''

@author:FC_LAMP

'''
import urllib2,urllib,socket
import os,re,threading,Queue
import cookielib,time,Image as image
import StringIO
#30 S请求
socket.setdefaulttimeout(30)

#详情页
class spiderDetailThread(threading.Thread):
header = {
'User-Agent':'Mozilla/5.0 (Windows NT 5.1; rv:6.0.2) Gecko/20100101 Firefox/6.0.2',
'Referer':'http://www.xxx.com' #这里是某图片网站
}
dir_path = 'D:/test/'

def __init__(self,queue):
threading.Thread.__init__(self)
cookie = cookielib.CookieJar()
cookieproc = urllib2.HTTPCookieProcessor(cookie)
urllib2.install_opener(urllib2.build_opener(cookieproc))
self.queue = queue
self.dir_path = dir_address

def run(self):
while True:
urls = self.queue.get()
for url in urls:
res = urllib2.urlopen(urllib2.Request(url=url,headers=self.header)).read()
patt = re.compile(r'<title>([^<]+)<\/title>',re.I)
patt = patt.search(res)
if patt==None:
continue

#获取TITLE
title = patt.group(1).split('_')[0]#'abc/\\:*?"<>|'
for i in ['\\','/',':','*','?','"',"'",'<','>','|']:
title=title.replace(i,'')
title = unicode(title,'utf-8').encode('gbk')
print title
#获取图片
cid = url.split('/')[-1].split('c')[-1].split('.')[0]
patt = re.compile(r'new\s+Array\(".*?<div[^>]+>(.*?)<\/div>"\)',re.I|re.S)
patt =patt.search(res)
if not patt:
continue

patt = patt.group(1)
src_patt = re.compile(r'.*?src=\'(.*?)\'.*?',re.I|re.S)
src_patt = src_patt.findall(patt)
if not src_patt:
continue

#创建目录
try:
path = os.path.join(self.dir_path,title)
if not os.path.exists(path):
os.makedirs(path)
except Exception as e:
pass
if not os.path.exists(path):
continue

for src in src_patt:
name = src.split('/')[-1]
#小图
s_path = os.path.join(path,name)
img = urllib2.urlopen(src).read()
im = image.open(StringIO.StringIO(img))
im.save(s_path)
#中图
src = src.replace('_s.','_r.')
name = src.split('/')[-1]
m_path = os.path.join(path,name)
img = urllib2.urlopen(src).read()
im = image.open(StringIO.StringIO(img))
im.save(m_path)
#大图
src = src.replace('smallcase','case')
src = src.replace('_r.','.')
name = src.split('/')[-1]
b_path = os.path.join(path,name)
img = urllib2.urlopen(src).read()
im = image.open(StringIO.StringIO(img))
im.save(b_path)

self.queue.task_done()

#例表页
class spiderlistThread(threading.Thread):
header = {
'User-Agent':'Mozilla/5.0 (Windows NT 5.1; rv:6.0.2) Gecko/20100101 Firefox/6.0.2',
'Referer':'http://www.xxx.com' #这里某图片网站
}

def __init__(self,queue,url):
threading.Thread.__init__(self)
cookie = cookielib.CookieJar()
cookieproc = urllib2.HTTPCookieProcessor(cookie)
urllib2.install_opener(urllib2.build_opener(cookieproc))
self.queue = queue
self.url = url

def run(self):
i = 1
while 1:
url = '%slist0-%d.html'%(self.url,i)
res = urllib2.urlopen(urllib2.Request(url=url,headers=self.header)).read()
patt = re.compile(r'<ul\s+id="container"[^>]+>(.*?)<\/ul>',re.I|re.S)
patt = patt.search(res)
if not patt:
break
else:
res = patt.group(1)
patt = re.compile(r'<label\s+class="a">.*?href="(.*?)".*?<\/label>',re.I|re.S)
patt = patt.findall(res)
if not patt:
break
self.queue.put(patt)
i+=1
time.sleep(3)

self.queue.task_done()


'''
多线程图片抓取
'''
if __name__=='__main__':
print unicode('---=======图片抓取=====----\n先请输入图片的保存地址(一定要是像这样的路径：D:/xxx/ 不然会出现一些未知错误)。\n若不输入,则默认保存在D:/test/ 文件夹会自动创建','utf-8').encode('gbk')
dir_address = raw_input(u'地址(回车确定)：'.encode('gbk')).strip()
print unicode('抓取工作马上开始.......','utf-8').encode('gbk')
if not dir_address:
dir_address = 'D:/test/'
if not os.path.exists(dir_address):
#试着创建目录(多级)
try:
os.makedirs(dir_address)
except Exception as e:
raise Exception(u'无法创建目录%s'%(dir_address))

url = 'http://www.xxx.com/' #这里是某图片网站
queue = Queue.Queue()
t1 = spiderlistThread(queue,url)
t1.setDaemon(True)
t1.start()

t2 = spiderDetailThread(queue)
t2.setDaemon(True)
t2.start()

while 1:
pass

```
