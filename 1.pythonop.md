#Python 运维相关事宜练习

##Python下自动化管理Mysql数据库

##python分析nginx大日志文件


```
#log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
'$status $body_bytes_sent "$http_referer" '
'"$http_user_agent" "$http_x_forwarded_for"';

$remote_addr客户端的ip地址（如果中间有代理服务器那么这里显示的ip就为代理服务器的ip地址）
$remote_user用于记录远程客户端的用户名称（一般为“-”）
$time_local用于记录访问时间和时区
$request用于记录请求的url以及请求方法$status响应状态码
$body_bytes_sent给客户端发送的文件主体内容大小
$http_user_agent用户所使用的代理（一般为浏览器）
$http_x_forwarded_for可以记录客户端IP，通过代理服务器来记录客户端的ip地址
$http_referer可以记录用户是从哪个链接访问过来的

demo data

182.19.31.129 - - [2013-08-13T00:00:01-07:00] "GET /css/anniversary.css HTTP/1.1" 304 0 "http://www.chlinux.net/" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1500.95 Safari/537.36" "-"

```

- 进行一些基本的统计
```
  
import os  
import fileinput  
import re  
  
#日志的位置  
dir_log  = r"D:\python cmd\nginxlog"  
  
#使用的nginx默认日志格式$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" "$http_x_forwarded_for"'  
#日志分析正则表达式  
  
#203.208.60.230   
ipP = r"?P<ip>[\d.]*";  
  
#[21/Jan/2011:15:04:41 +0800]  
timeP = r"""?P<time>\[           #以[开始 
            [^\[\]]* #除[]以外的任意字符  防止匹配上下个[]项目(也可以使用非贪婪匹配*?)  不在中括号里的.可以匹配换行外的任意字符  *这样地重复是"贪婪的“ 表达式引擎会试着重复尽可能多的次数。 
            \]           #以]结束 
        """  
  
#"GET /EntpShop.do?method=view&shop_id=391796 HTTP/1.1"  
requestP = r"""?P<request>\"          #以"开始 
            [^\"]* #除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?) 
            \"          #以"结束 
            """  
  
statusP = r"?P<status>\d+"  
  
bodyBytesSentP = r"?P<bodyByteSent>\d+"  
  
#"http://test.myweb.com/myAction.do?method=view&mod_id=&id=1346"  
referP = r"""?P<refer>\"          #以"开始 
            [^\"]* #除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?) 
            \"          #以"结束 
        """  
  
#"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"'  
userAgentP = r"""?P<userAgent>\"              #以"开始 
        [^\"]* #除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?) 
        \"              #以"结束 
            """  
  
#原理：主要通过空格和-来区分各不同项目，各项目内部写各自的匹配表达式  
nginxLogPattern = re.compile(r"(%s)\ -\ -\ (%s)\ (%s)\ (%s)\ (%s)\ (%s)\ (%s)" %(ipP, timeP, requestP, statusP, bodyBytesSentP, referP, userAgentP), re.VERBOSE)  
  
def processDir(dir_proc):  
    for file in os.listdir(dir_proc):  
        if os.path.isdir(os.path.join(dir_proc, file)):  
            print "WARN:%s is a directory" %(file)  
            processDir(os.path.join(dir_proc, file))  
            continue  
  
        if not file.endswith(".log"):  
            print "WARN:%s is not a log file" %(file)  
            continue  
  
        print "INFO:process file %s" %(file)  
        for line in fileinput.input(os.path.join(dir_proc, file)):  
            matchs = nginxLogPattern.match(line)  
            if matchs!=None:  
                allGroups = matchs.groups()  
                ip = allGroups[0]  
                time = allGroups[1]  
                request = allGroups[2]  
                status =  allGroups[3]  
                bodyBytesSent = allGroups[4]  
                refer = allGroups[5]  
#                userAgent = allGroups[6]  
                userAgent = matchs.group("userAgent")  
                print userAgent  
                  
                #统计HTTP状态码的数量  
                GetResponseStatusCount(userAgent)  
                #在这里补充其他任何需要的分析代码  
            else:  
                raise Exception  
                  
        fileinput.close()  
  
allStatusDict = {}  
#统计HTTP状态码的数量  
def GetResponseStatusCount(status):  
    if allStatusDict.has_key(status):  
        allStatusDict[status] += 1;  
    else:  
        allStatusDict[status] = 1;  
      
          
if __name__ == "__main__":  
    processDir(dir_log)  
    print allStatusDict  
    #根据值进行排序（倒序）  
    print sorted(allStatusDict.items(), key=lambda d:d[1], reverse=True)  
    print "done, python is great!"  
```

- 塞入mysql

``` 
#!/usr/bin/env python
#coding:utf8
import os
import fileinput
import re
import sys
import MySQLdb
#日志的位置
logfile=open("access_20130812.log")
#使用的nginx默认日志格式$remote_addr - $remote_user [$time_local] "$request" $status $body_bytes_sent "$http_referer" "$http_user_agent" "$http_x_forwarded_for"'
#日志分析正则表达式
#203.208.60.230
ipP = r"?P<ip>[\d.]*"
#以[开始,除[]以外的任意字符 防止匹配上下个[]项目(也可以使用非贪婪匹配*?) 不在中括号里的.可以匹配换行外的任意字符 *这样地重复是"贪婪的“ 表达式引擎会试着重复尽可能多的次数。#以]结束
#[21/Jan/2011:15:04:41 +0800]
timeP = r"""?P<time>\[[^\[\]]*\]"""
#以"开始, #除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?),#以"结束
#"GET /EntpShop.do?method=view&shop_id=391796 HTTP/1.1"
#"GET /EntpShop.do?method=view&shop_id=391796 HTTP/1.1"
requestP = r"""?P<request>\"[^\"]*\""""
statusP = r"?P<status>\d+"
bodyBytesSentP = r"?P<bodyByteSent>\d+"
#以"开始, 除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?),#以"结束
#"http://test.myweb.com/myAction.do?method=view&mod_id=&id=1346"
referP = r"""?P<refer>\"[^\"]*\""""
#以"开始, 除双引号以外的任意字符 防止匹配上下个""项目(也可以使用非贪婪匹配*?),以"结束
#"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"'
userAgentP = r"""?P<userAgent>\"[^\"]*\""""
#以(开始, 除双引号以外的任意字符 防止匹配上下个()项目(也可以使用非贪婪匹配*?),以"结束
#(compatible; Googlebot/2.1; +http://www.google.com/bot.html)"'
userSystems = re.compile(r'\([^\(\)]*\)')
#以"开始，除双引号以外的任意字符防止匹配上下个""项目(也可以使用非贪婪匹配*?),以"结束
userlius = re.compile(r'[^\)]*\"')
#原理：主要通过空格和-来区分各不同项目，各项目内部写各自的匹配表达式
nginxLogPattern = re.compile(r"(%s)\ -\ -\ (%s)\ (%s)\ (%s)\ (%s)\ (%s)\ (%s)" %(ipP, timeP, requestP, statusP, bodyBytesSentP, referP, userAgentP), re.VERBOSE)
#数据库连接信息
conn=MySQLdb.connect(host='192.168.1.22',user='test',passwd='pass',port=3306,db='python')
cur=conn.cursor()
sql = "INSERT INTO python.test VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s)"
while True:
    line = logfile.readline()
    if not line:break
    matchs = nginxLogPattern.match(line)
    if matchs != None:
        allGroup = matchs.groups()
        ip = allGroup[0]
        time = allGroup[1]
        request = allGroup[2]
        status = allGroup[3]
        bodyBytesSent = allGroup[4]
        refer = allGroup[5]
        userAgent = allGroup[6]
        Time = time.replace('T',' ')[1:-7]
        if len(userAgent) > 20:
            userinfo = userAgent.split(' ')
            userkel =  userinfo[0]
            try:
                usersystem = userSystems.findall(userAgent)
                usersystem = usersystem[0]
                print usersystem
                userliu = userlius.findall(userAgent)
                value = [ip,Time,request,status,bodyBytesSent,refer,userkel,usersystem,userliu[1]]
                conn.commit()
                print value
            except IndexError:
                userinfo = userAgent
                value = [ip,Time,request,status,bodyBytesSent,refer,userinfo,"",""]
        else:
            useraa = userAgent
            value = [ip,Time,request,status,bodyBytesSent,refer,useraa,"",""]
    try:
        result = cur.execute(sql,value)
        #conn.commit()
        print result
    except MySQLdb.Error,e:
        print "Mysql Error %d: %s" % (e.args[0], e.args[1])
conn.commit()
conn.close()
```

- 复杂实例

```
#!/usr/bin/python
#coding:utf8
import threading     #载入多线程模块
import time          #载入时间模块
import os            #载入os模块
import shutil        #载入shutil模块
import re            #载入re正则模块
fuhao=os.linesep     #换行符
start_time=int(time.strftime('%H%M%S'))  #执行程序开始时间
print start_time
def count_cpu_heshu():                      #统计cpu核数函数
    file=open('/proc/cpuinfo')
    cpu_sum=[]
    for line in file.readlines():
        cpu_he=re.findall('^processor',line)
        if len(cpu_he)==1:
           cpu_sum.append(cpu_he)
        else:
           continue
    file.close()
    return len(cpu_sum)                    #返回cpu函数
def count_memory_size():                   #统计系统内存大小函数
    mem_sum=int(os.popen("free -m|awk  '{print $2}'|sed -n '2p'").readline().strip())  #统计内存的shell
    return mem_sum                                                                     #返回内存大小
def nginx_log_fenge():                    #因nginx日志太大，需要按500M分割，建立此函数进行分割日志
    if os.path.exists('/data/logs/nginx_tmp/')!=True:       #分割日志的临时目录
       os.makedirs('/data/logs/nginx_tmp/')
    if os.path.exists('/data/logs/nginx_tmp_binfa/')!=True: #并发目录
       os.makedirs('/data/logs/nginx_tmp_binfa/')
    if os.path.exists('/data/logs/nginx_tmp_txt01/')!=True:  #time记录txt目录 
       os.makedirs('/data/logs/nginx_tmp_txt01/')
    if os.path.exists('/data/logs/nginx_tmp_txt02/')!=True:  #url记录txt目录 
       os.makedirs('/data/logs/nginx_tmp_txt02/')
    if os.path.exists('/data/logs/nginx_tmp_chuli/')!=True:  #处理所有txt目录
       os.makedirs('/data/logs/nginx_tmp_chuli/')
    nginx_log_name=os.listdir('/data/logs/nginx_log')[0].strip()  #切割日志名
    nginx_file='/data/logs/nginx_log/%s' %nginx_log_name           #切割日志名及路径
    file=open(nginx_file)                   #nginx日志文件路径
    sizehint=int(count_memory_size() / count_cpu_heshu() * 0.5 * 1024 * 1024)              #此数字是按字节进行计算，这里大小为内存除以cpu核数剩以0.5得到的结果为500M
    position=0                               #当前位置为0
    file_num=1                               #分割文件名默认加1
    while True:                             #当条件为真执行，为假退出。
          lines = file.readlines(sizehint)   #读文件
          file_name='/data/logs/nginx_tmp/dd_access%d' %file_num  #分割成功的文件名
          file01=open(file_name,'w')       #写文件
          file01.writelines(lines)         #把读取的单个1G文件写到文件名中
          if  file.tell() - position > 0: #如果分割的位置大于默认位置就继续执行，否则就退出。
              position = file.tell()      #替换位置
              file_num=file_num+1
              continue
          else:
              break
    file.close()
    file01.close()
    os.remove(file_name)
    time.sleep(300)
def nginx_log_time_count(file_name01):         #nginx分析日志函数
    file_name='/data/logs/nginx_tmp_binfa/%s' %file_name01    #并发日志名
    file_txt='/data/logs/nginx_tmp_txt01/%s.txt' %file_name01        #执行shell结果保存目录
    cmd="awk '{print $4}' %s|awk -F / '{print $NF}'|awk -F : '{print $2$3$4$5}'  2>/dev/null|sort 2>/dev/null|uniq -c 2>/dev/null|sort -nr 2>/dev/null|head -n 1 > %s" %(file_name,file_txt)  #分析脚本
    os.system(cmd)  #执行shell命令
    fuhao_cmd='%s' %fuhao
    f=open(file_txt)
    f1=open('/data/logs/nginx_tmp_chuli/time_sum.txt','a')
    for line in f.readlines():
         time_single_max= line.split()[0]   #单个文件连接数
         f1.writelines(time_single_max)
         f1.write(fuhao_cmd)
    f.close()
    f1.close()
def nginx_log_url_count(file_name01):         #nginx分析日志函数
    file_name='/data/logs/nginx_tmp_binfa/%s' %file_name01  #并发日志名
    file_txt='/data/logs/nginx_tmp_txt02/%s.txt' %file_name01   #执行shell结果保存目录
    cmd="awk '{print $7}' %s  2>/dev/null|sort 2>/dev/null|uniq -c 2>/dev/null|sort -rn 2>/dev/null|head -n 200 > %s " %(file_name,file_txt)  #分析脚本
    os.system(cmd)  #执行shell命令
    fuhao_cmd='%s' %fuhao
    f=open(file_txt)
    f1=open('/data/logs/nginx_tmp_chuli/url_sum.txt','a')
    for line in f.readlines():          #把url_status里面每一行值以列表方法增加到url_count列表里面
        f1.writelines(line.strip())
        f1.write(fuhao_cmd)
    f.close()
    f1.close()
def dxc_call_time_count():                       #多线程调用分析日志函数
    file_name_read=[]                        #文件名读取列表
    f=os.listdir('/data/logs/nginx_tmp_binfa/')   #显示data/logs/nginx_tmp/目录下所有文件
    for read_filename in f:
        filename_chuancan=read_filename.strip()  #单个文件名
        filename=threading.Thread(target=nginx_log_time_count,args=(filename_chuancan,))  #建立多线程
        file_name_read.append(filename)         #添加线程到file_name_read列表
    filename_sum=range(len(file_name_read))     #统计文件名数量
    for line in filename_sum:
        file_name_read[line].start()            #启动线程
    for line in filename_sum:
        file_name_read[line].join()             #等待多线程结束后，就结束进程。
def dxc_call_url_count():                       #多线程调用分析日志函数
    file_name_read=[]                        #文件名读取列表
    f=os.listdir('/data/logs/nginx_tmp_binfa/')   #显示data/logs/nginx_tmp/目录下所有文件
    for read_filename in f:
        filename_chuancan=read_filename.strip()  #单个文件名
        filename=threading.Thread(target=nginx_log_url_count,args=(filename_chuancan,))  #建立多线程
        file_name_read.append(filename)         #添加线程到file_name_read列表
    filename_sum=range(len(file_name_read))     #统计文件名数量
    for line in filename_sum:
        file_name_read[line].start()            #启动线程
    for line in filename_sum:
        file_name_read[line].join()             #等待多线程结束后，就结束进程。
def time_count_chuli():                         #time处理函数
    f=open('/data/logs/nginx_tmp_chuli/time_sum.txt')
    time_max=[]
    for count in f:
       time_max.append(int(count.strip()))
    f.close()
    return max(time_max)
def url_count_chuli():                          #url处理函数
    f=open('/data/logs/nginx_tmp_chuli/url_sum.txt')
    url_max=[]
    for count in f:
        url_max.append(count.split())
    f.close()
    return url_max
def write_report_email():                       #写文件用来发email
    fuhao_cmd='%s' %fuhao
    time_max=time_count_chuli()                 #接受time处理函数返回的结果
    url_max=url_count_chuli()                   #接受url处理函数返回的结果
    file=open('/data/logs/nginx_log_email_tmp.txt','w')
    file.write("nginx单秒的最大请求数为:%d" %time_max)
    file.write(fuhao_cmd)
    file.write('nginx连接数TOP100排序')
    file.write(fuhao_cmd)
    new_dict={}                       #定义一字典用来统计连接重复数,得到字典结果为连接地址:连接重复数
    for i in url_max:                 #遍历url_max列表
         new_dict[i[1]] = new_dict.get(i[1],0) + int(i[0])    #i[1]表示连接地址,i[0]表示连接重复数,new_dict[i[1]]表示把列表中的地址重复数与地址连接交换,如果连接相同，就累加连接重复数.
    n_dict = {}                       #定义一字典用来恢复原来的连接重复数:连接地址
    for k in new_dict:                #遍历new_dict字典
         n_dict[new_dict[k]] = k      #k表示连接地址,new_dict[k]表示连接重复数,最后n_dict结果为连接重复数:连接地址
    url_count=sorted(n_dict.iteritems(),key=lambda dict1:dict1[0],reverse=True) #对字典进行排序
    for line in url_count:
        file.write('连接重复数:')
        file.write(str(line[0]))              #把连接重复数写到日志临时文件
        file.write('   ')
        file.write('http://d.m1905.com')      #写连接头文件
        file.write(str(line[1]))              #把连接地址写到日志临时文件
        file.write(fuhao_cmd)
    file.close()
    file=open('/data/logs/nginx_log_email_tmp.txt','r')   #读取日志临时文件
    row=0
    file01=open('/data/logs/nginx_log_email.txt','w')     #写文件
    for line in file.readlines():
        row=row+1                                         #row表示行数
        if row <= 102:                                    #读取文件到102行，大于102行就退出
           file01.write(line)
        else:
           break
    file.close()
    file01.close()
    os.remove('/data/logs/nginx_log_email_tmp.txt')       #删除日志临时文件  
    os.remove('/data/logs/nginx_tmp_chuli/time_sum.txt')       #删除time_sum文件 
    os.remove('/data/logs/nginx_tmp_chuli/url_sum.txt')       #删url_sum文件  
def rmdir_nginx_log_mulu():       #清空日志目录函数
    shutil.rmtree('/data/logs/nginx_tmp/')  #清空日志临时目录，供新日志存放
    os.mkdir('/data/logs/nginx_tmp/')
    shutil.rmtree('/data/logs/nginx_log/')  #清空日志目录，供新日志存放
    os.mkdir('/data/logs/nginx_log')
def main():
   shutil.rmtree('/data/logs/nginx_tmp_chuli')  #清空日志临时目录，供新日志存放
   os.mkdir('/data/logs/nginx_tmp_chuli')
   cpu_he=count_cpu_heshu()                                       #cpu核数
   while len(os.listdir('/data/logs/nginx_tmp/'))>0:   #动态统计分割日志文件个数
 f=os.listdir('/data/logs/nginx_tmp/')          #动态统计分割日志文件
 key=0                                                       #默认key为0
 while (key<=cpu_he-1 and key<len(f)):      #对cpu核数进行对比
         name = '/data/logs/nginx_tmp/%s' %f[key]    #日志文件名
         shutil.move(name,'/data/logs/nginx_tmp_binfa/')  #移动日志文件，为了减少负载太高
         key=key+1
 dxc_call_time_count()
        dxc_call_url_count() 
 shutil.rmtree('/data/logs/nginx_tmp_binfa/')
 os.mkdir('/data/logs/nginx_tmp_binfa/')
        shutil.rmtree('/data/logs/nginx_tmp_txt01/')
        os.mkdir('/data/logs/nginx_tmp_txt01/')
        shutil.rmtree('/data/logs/nginx_tmp_txt02/')
        os.mkdir('/data/logs/nginx_tmp_txt02/')
   write_report_email()
   rmdir_nginx_log_mulu() 
nginx_log_fenge()
main()
stop_time=int(time.strftime('%H%M%S'))
print stop_time
```

## 多线程

- 多线程socket server

```
#coding:utf-8
import socket
import sys
import time
import Queue
import threading

host = 'localhost'
port = 8000

#创建socket对象
s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)

#绑定一个特定地址，端口
try:
    s.bind((host,port))
except  Exception as e :
    print 'Bind Failed:%s'%(str(e))
    sys.exit()
print 'Socket bind complete!!'

#监听连接
s.listen(10) #最大连接数10

#创建连接队列
queue = Queue.Queue()

#创建线程
class TaskThread(threading.Thread):
    def __init__(self):
        threading.Thread.__init__(self)
        
    def run(self):
        while 1:
            t = queue.get()
            t.send('welecome.....')
            #接收数据
            client_data = t.recv(1024)
            t.sendall(client_data)
            #释放资源
            #t.close()            
        

#接受连接
while 1:

    #将连接放入队列
    conn,addr = s.accept()
    print 'Connected from %s:%s'%(addr[0],str(addr[1]))
    queue.put(conn)

    #生成线程池
    th = TaskThread()
    th.setDaemon(True)
    th.start()

    queue.join()
s.close()

```

- 多线程下载图片
```
#coding:utf-8

'''

@author:FC_LAMP

'''
import urllib2,urllib,socket
import os,re,threading,Queue
import cookielib,time,Image as image
import StringIO
#30 S请求
socket.setdefaulttimeout(30)

#详情页
class spiderDetailThread(threading.Thread):
header = {
'User-Agent':'Mozilla/5.0 (Windows NT 5.1; rv:6.0.2) Gecko/20100101 Firefox/6.0.2',
'Referer':'http://www.xxx.com' #这里是某图片网站
}
dir_path = 'D:/test/'

def __init__(self,queue):
threading.Thread.__init__(self)
cookie = cookielib.CookieJar()
cookieproc = urllib2.HTTPCookieProcessor(cookie)
urllib2.install_opener(urllib2.build_opener(cookieproc))
self.queue = queue
self.dir_path = dir_address

def run(self):
while True:
urls = self.queue.get()
for url in urls:
res = urllib2.urlopen(urllib2.Request(url=url,headers=self.header)).read()
patt = re.compile(r'<title>([^<]+)<\/title>',re.I)
patt = patt.search(res)
if patt==None:
continue

#获取TITLE
title = patt.group(1).split('_')[0]#'abc/\\:*?"<>|'
for i in ['\\','/',':','*','?','"',"'",'<','>','|']:
title=title.replace(i,'')
title = unicode(title,'utf-8').encode('gbk')
print title
#获取图片
cid = url.split('/')[-1].split('c')[-1].split('.')[0]
patt = re.compile(r'new\s+Array\(".*?<div[^>]+>(.*?)<\/div>"\)',re.I|re.S)
patt =patt.search(res)
if not patt:
continue

patt = patt.group(1)
src_patt = re.compile(r'.*?src=\'(.*?)\'.*?',re.I|re.S)
src_patt = src_patt.findall(patt)
if not src_patt:
continue

#创建目录
try:
path = os.path.join(self.dir_path,title)
if not os.path.exists(path):
os.makedirs(path)
except Exception as e:
pass
if not os.path.exists(path):
continue

for src in src_patt:
name = src.split('/')[-1]
#小图
s_path = os.path.join(path,name)
img = urllib2.urlopen(src).read()
im = image.open(StringIO.StringIO(img))
im.save(s_path)
#中图
src = src.replace('_s.','_r.')
name = src.split('/')[-1]
m_path = os.path.join(path,name)
img = urllib2.urlopen(src).read()
im = image.open(StringIO.StringIO(img))
im.save(m_path)
#大图
src = src.replace('smallcase','case')
src = src.replace('_r.','.')
name = src.split('/')[-1]
b_path = os.path.join(path,name)
img = urllib2.urlopen(src).read()
im = image.open(StringIO.StringIO(img))
im.save(b_path)

self.queue.task_done()

#例表页
class spiderlistThread(threading.Thread):
header = {
'User-Agent':'Mozilla/5.0 (Windows NT 5.1; rv:6.0.2) Gecko/20100101 Firefox/6.0.2',
'Referer':'http://www.xxx.com' #这里某图片网站
}

def __init__(self,queue,url):
threading.Thread.__init__(self)
cookie = cookielib.CookieJar()
cookieproc = urllib2.HTTPCookieProcessor(cookie)
urllib2.install_opener(urllib2.build_opener(cookieproc))
self.queue = queue
self.url = url

def run(self):
i = 1
while 1:
url = '%slist0-%d.html'%(self.url,i)
res = urllib2.urlopen(urllib2.Request(url=url,headers=self.header)).read()
patt = re.compile(r'<ul\s+id="container"[^>]+>(.*?)<\/ul>',re.I|re.S)
patt = patt.search(res)
if not patt:
break
else:
res = patt.group(1)
patt = re.compile(r'<label\s+class="a">.*?href="(.*?)".*?<\/label>',re.I|re.S)
patt = patt.findall(res)
if not patt:
break
self.queue.put(patt)
i+=1
time.sleep(3)

self.queue.task_done()


'''
多线程图片抓取
'''
if __name__=='__main__':
print unicode('---=======图片抓取=====----\n先请输入图片的保存地址(一定要是像这样的路径：D:/xxx/ 不然会出现一些未知错误)。\n若不输入,则默认保存在D:/test/ 文件夹会自动创建','utf-8').encode('gbk')
dir_address = raw_input(u'地址(回车确定)：'.encode('gbk')).strip()
print unicode('抓取工作马上开始.......','utf-8').encode('gbk')
if not dir_address:
dir_address = 'D:/test/'
if not os.path.exists(dir_address):
#试着创建目录(多级)
try:
os.makedirs(dir_address)
except Exception as e:
raise Exception(u'无法创建目录%s'%(dir_address))

url = 'http://www.xxx.com/' #这里是某图片网站
queue = Queue.Queue()
t1 = spiderlistThread(queue,url)
t1.setDaemon(True)
t1.start()

t2 = spiderDetailThread(queue)
t2.setDaemon(True)
t2.start()

while 1:
pass

```
